# -*- coding: utf-8 -*-
"""Dragonshacks2022 DALLÂ·E mini - Inference pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k3Om0C1QSW93jYTLno6sosjIJoVjQXEF

# DALLÂ·E mini - Inference pipeline

*Generate images from a text prompt*

<img src="https://github.com/borisdayma/dalle-mini/blob/main/img/logo.png?raw=true" width="200">

This notebook illustrates [DALLÂ·E mini](https://github.com/borisdayma/dalle-mini) inference pipeline.

Just want to play? Use [the demo](https://huggingface.co/spaces/flax-community/dalle-mini).

For more understanding of the model, refer to [the report](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini--Vmlldzo4NjIxODA).

## ðŸ› ï¸ Installation and set-up
"""

# Install required libraries
!pip install -q git+https://github.com/huggingface/transformers.git
!pip install -q git+https://github.com/patil-suraj/vqgan-jax.git
!pip install -q git+https://github.com/borisdayma/dalle-mini.git

"""We load required models:
* dalleÂ·mini for text to encoded images
* VQGAN for decoding images
* CLIP for scoring predictions
"""

# Model references

# dalle-mini
DALLE_MODEL = "dalle-mini/dalle-mini/wzoooa1c:latest"  # can be wandb artifact or ðŸ¤— Hub or local folder or google bucket
DALLE_COMMIT_ID = None

# VQGAN model
VQGAN_REPO = "dalle-mini/vqgan_imagenet_f16_16384"
VQGAN_COMMIT_ID = "e93a26e7707683d349bf5d5c41c5b0ef69b677a9"

# CLIP model
CLIP_REPO = "openai/clip-vit-large-patch14"
CLIP_COMMIT_ID = None

import jax
import jax.numpy as jnp

# check how many devices are available
jax.local_device_count()

# Load models & tokenizer
from dalle_mini import DalleBart, DalleBartProcessor
from vqgan_jax.modeling_flax_vqgan import VQModel
from transformers import CLIPProcessor, FlaxCLIPModel

# Load dalle-mini
model = DalleBart.from_pretrained(DALLE_MODEL, revision=DALLE_COMMIT_ID)

# Load VQGAN
vqgan = VQModel.from_pretrained(VQGAN_REPO, revision=VQGAN_COMMIT_ID)

# Load CLIP
clip = FlaxCLIPModel.from_pretrained(CLIP_REPO, revision=CLIP_COMMIT_ID)
clip_processor = CLIPProcessor.from_pretrained(CLIP_REPO, revision=CLIP_COMMIT_ID)

"""Model parameters are replicated on each device for faster inference."""

from flax.jax_utils import replicate

model._params = replicate(model.params)
vqgan._params = replicate(vqgan.params)
clip._params = replicate(clip.params)

"""Model functions are compiled and parallelized to take advantage of multiple devices."""

from functools import partial

# model inference
@partial(jax.pmap, axis_name="batch", static_broadcasted_argnums=(3, 4, 5, 6))
def p_generate(
    tokenized_prompt, key, params, top_k, top_p, temperature, condition_scale
):
    return model.generate(
        **tokenized_prompt,
        prng_key=key,
        params=params,
        top_k=top_k,
        top_p=top_p,
        temperature=temperature,
        condition_scale=condition_scale,
    )


# decode image
@partial(jax.pmap, axis_name="batch")
def p_decode(indices, params):
    return vqgan.decode_code(indices, params=params)


# score images
@partial(jax.pmap, axis_name="batch")
def p_clip(inputs, params):
    logits = clip(params=params, **inputs).logits_per_image
    return logits

"""Keys are passed to the model on each device to generate unique inference per device."""

import random

# create a random key
seed = random.randint(0, 2**32 - 1)
key = jax.random.PRNGKey(seed)

"""## ðŸ– Text Prompt

Our model requires processing prompts.
"""

from dalle_mini import DalleBartProcessor

processor = DalleBartProcessor.from_pretrained(DALLE_MODEL, revision=DALLE_COMMIT_ID)

"""Let's define a text prompt."""

prompt = "ducks crossing the road"

tokenized_prompt = processor([prompt])

"""Finally we replicate it onto each device."""

tokenized_prompt = replicate(tokenized_prompt)

"""## ðŸŽ¨ Generate images

We generate images using dalle-mini model and decode them with the VQGAN.
"""

# number of predictions
n_predictions = 16

# We can customize top_k/top_p used for generating samples
gen_top_k = None
gen_top_p = None
temperature = 0.85
cond_scale = 3.0

from flax.training.common_utils import shard_prng_key
import numpy as np
from PIL import Image
from tqdm.notebook import trange

# generate images
images = []
for i in trange(max(n_predictions // jax.device_count(), 1)):
    # get a new key
    key, subkey = jax.random.split(key)
    # generate images
    encoded_images = p_generate(
        tokenized_prompt,
        shard_prng_key(subkey),
        model.params,
        gen_top_k,
        gen_top_p,
        temperature,
        cond_scale,
    )
    # remove BOS
    encoded_images = encoded_images.sequences[..., 1:]
    # decode images
    decoded_images = p_decode(encoded_images, vqgan.params)
    decoded_images = decoded_images.clip(0.0, 1.0).reshape((-1, 256, 256, 3))
    for img in decoded_images:
        images.append(Image.fromarray(np.asarray(img * 255, dtype=np.uint8)))

"""Let's calculate their score with CLIP."""

from flax.training.common_utils import shard

# get clip scores
clip_inputs = clip_processor(
    text=[prompt] * jax.device_count(),
    images=images,
    return_tensors="np",
    padding="max_length",
    max_length=77,
    truncation=True,
).data
logits = p_clip(shard(clip_inputs), clip.params)
logits = logits.squeeze().flatten()

"""Let's display images ranked by CLIPÂ score."""

print(f"Prompt: {prompt}\n")
i = 0
for idx in logits.argsort()[::-1]:
    display(images[idx])
    print(f"Score: {logits[idx]:.2f}\n")
    images[idx].save(f"dallE{i}.jpg")
    i += 1

!pip install flask-ngrok
!pip install flask==0.12.2  # Newer versions of flask don't work in Colab
                            # See https://github.com/plotly/dash/issues/257

!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz
!tar -xvf /content/ngrok-stable-linux-amd64.tgz

!./ngrok authtoken ##inserauthtoken here

##RUN THIS FIRST
from google.colab import auth
auth.authenticate_user()

# https://cloud.google.com/resource-manager/docs/creating-managing-projects
project_id = 'aiot-fit-xlab'
!gcloud config set project {project_id}

def storetexttogcp(text, storyname):
  fname = f'/tmp/story-{storyname}.txt'
  with open(fname, 'w') as f:
    f.write(text)

  bucket_name = 'dragonhacks2022'


  # Copy the file to our bucket.
  # Full reference: https://cloud.google.com/storage/docs/gsutil/commands/cp
  !gsutil cp {fname} gs://{bucket_name}/

  return f"https://storage.googleapis.com/dragonhacks2022/story-{storyname}.txt"

def storeimagetogcp(fname):
  # fname = f'/tmp/story-{storyname}.txt'
  # with open(fname, 'w') as f:
  #   f.write(text)

  bucket_name = 'dragonhacks2022'


  # Copy the file to our bucket.
  # Full reference: https://cloud.google.com/storage/docs/gsutil/commands/cp
  !gsutil cp {fname} gs://{bucket_name}/

  return f"https://storage.googleapis.com/dragonhacks2022/{fname}"

# flask_ngrok_example.py
from flask import Flask, redirect, url_for, request
from flask_ngrok import run_with_ngrok
import json
import requests
import time

from flax.training.common_utils import shard_prng_key
import numpy as np
from PIL import Image
from tqdm.notebook import trange
from flax.training.common_utils import shard


app = Flask(__name__)
run_with_ngrok(app)  # Start ngrok when app is run

@app.route("/")
def hello():
    return "Hello World!"


@app.route("/getphrases", methods = ['GET', 'POST'])
def getPhrases():
  if request.method == 'POST':
    words = json.loads(request.data)
    sentences = createSentences(words["words"])
    story = generateStory(words["words"])
    sname = storetexttogcp(story, words["words"][0]+str(int(time.time())) )
    # write_to_blob(story,words["words"][0])
    return {"sentences":sentences,"story":story, "surl": sname}


@app.route("/testgen", methods = ['POST'])
def testgen():
  seed = random.randint(0, 2**32 - 1)
  key = jax.random.PRNGKey(seed)
  jst = json.loads(request.data)
  prompt = jst['sentence']

  # prompt = "ducks crossing the road"

  tokenized_prompt = processor([prompt])

  tokenized_prompt = replicate(tokenized_prompt)


  # number of predictions
  n_predictions = 16

  # We can customize top_k/top_p used for generating samples
  gen_top_k = None
  gen_top_p = None
  temperature = 0.85
  cond_scale = 3.0

  # generate images
  images = []
  for i in trange(max(n_predictions // jax.device_count(), 1)):
      # get a new key
      key, subkey = jax.random.split(key)
      # generate images
      encoded_images = p_generate(
          tokenized_prompt,
          shard_prng_key(subkey),
          model.params,
          gen_top_k,
          gen_top_p,
          temperature,
          cond_scale,
      )
      # remove BOS
      encoded_images = encoded_images.sequences[..., 1:]
      # decode images
      decoded_images = p_decode(encoded_images, vqgan.params)
      decoded_images = decoded_images.clip(0.0, 1.0).reshape((-1, 256, 256, 3))
      for img in decoded_images:
          images.append(Image.fromarray(np.asarray(img * 255, dtype=np.uint8)))

  # get clip scores
  clip_inputs = clip_processor(
      text=[prompt] * jax.device_count(),
      images=images,
      return_tensors="np",
      padding="max_length",
      max_length=77,
      truncation=True,
  ).data
  logits = p_clip(shard(clip_inputs), clip.params)
  logits = logits.squeeze().flatten()

  print(f"Prompt: {prompt}\n")
  i = 0
  filestoreurls = []
  for idx in logits.argsort()[::-1]:
      display(images[idx])
      print(f"Score: {logits[idx]:.2f}\n")
      images[idx].save(f"dallE{i}.jpg")

      fname = f"dallE{i}.jpg"

      filestoreurls.append(storeimagetogcp(fname))
      i += 1


  retjson = {}

  retjson['imurls'] = filestoreurls
  retjson['status'] = "images generated"

  return json.dumps(retjson)


if __name__ == '__main__':
    app.run()  # If address is in use, may need to terminate other sessions:
               # Runtime > Manage Sessions > Terminate Other Sessions
